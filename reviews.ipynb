{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO insert validation into training. Stop training as soon as validation test score starts to decrease\n",
    "# k-fold cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from torch.optim import AdamW \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Read in the data from train.txt\n",
    "with open(\"train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(\"test_just_reviews.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines_test = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to handle the labels\n",
    "def process(label): # labels can be \"TRUTHFULPOSITIVE\", \"DECEPTIVEPOSITIVE\",  \"TRUTHFULNEGATIVE\" or \"DECEPTIVENEGATIVE\"\n",
    "    if label == \"TRUTHFULPOSITIVE\":\n",
    "        return 0\n",
    "    elif label == \"DECEPTIVEPOSITIVE\":\n",
    "        return 1\n",
    "    elif label == \"TRUTHFULNEGATIVE\":\n",
    "        return 2\n",
    "    elif label == \"DECEPTIVENEGATIVE\":\n",
    "        return 3\n",
    "    else:\n",
    "        print(\"Error: label not found\")\n",
    "        return -1\n",
    "\n",
    "def convert(label_int):\n",
    "    if label_int == 0:\n",
    "        return \"TRUTHFULPOSITIVE\"\n",
    "    elif label_int == 1:\n",
    "        return \"DECEPTIVEPOSITIVE\"\n",
    "    elif label_int == 2:\n",
    "        return \"TRUTHFULNEGATIVE\"\n",
    "    elif label_int == 3:\n",
    "        return \"DECEPTIVENEGATIVE\"\n",
    "    else:\n",
    "        print(\"Error: label not found\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the lines into labels and texts\n",
    "train_labels = []\n",
    "train_texts = []\n",
    "for line in lines:\n",
    "    label, text = line.strip().split(\"\\t\")\n",
    "    train_labels.append(process(label))\n",
    "    train_texts.append(text)\n",
    "\n",
    "test_texts = []\n",
    "for line in lines_test:\n",
    "    text = line.strip()\n",
    "    test_texts.append(text)\n",
    "\n",
    "# train test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts[:500], train_labels[:500], test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "DECEPTIVEPOSITIVE\n",
      " If you are traveling to Chicago and need a place to stay, I highly recommend the Hotel Allegro Chicago. It was visiting time visiting Chicago for the first time on a business trip and a friend referred me to the Hotel Allegro. He said they were pet friendly,which was a huge plus. This is because I hate leaving my kitten muffin with a pet sitter every time I travel for business. Upon my arrival, I was met with friendly valet who treated me like I was the only customer who mattered. I walked in and saw a hotel like no other I have seen before or after. The lobby was decorated with modern furniture and paintings and looked like a place I could relax in. After I had entered and checked in, I saw the staircase, which looked like a testament to the staircase in the \"Titanic\". Frankly, I was amazed and I had not even entered my room yet. My room was a Queen Deluxe room and it was amazing. The bed had a blue headboard,which I have never seen before, and the colors worked together so well I seriously contemplated living there. The room service was exceptional and promptly brought the food I ordered. Later I wanted to get a drink and I enjoyed it at The Bar 312, which is conveniently located adjacent to the hotel. These are the specifics of my stay, but there is much more to be experienced that I did not mention if you visit the Hotel Allegro. In summary, they allow pets, the service is amazing, the decoration superb, and it is conveniently situated next to dining establishments. What more could a person ask for?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i = random.randint(0, len(train_texts)-1)\n",
    "print(len(train_texts))\n",
    "print(convert(train_labels[i]))\n",
    "print(train_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "[101, 2004, 1037, 6976, 20174, 2000, 3190, 1010, 1045, 3227, 2994, 2012, 1996, 11175, 2457, 1010, 2021, 2245, 1045, 2052, 3046, 1037, 2367, 24611, 3309, 1012, 1045, 2097, 2196, 2994, 2012, 2023, 3309, 2153, 1012, 1996, 2034, 2282, 2027, 2716, 2033, 2000, 1011, 1011, 2037, 3115, 2282, 1011, 1011, 2001, 1037, 1043, 10626, 7810, 9346, 1010, 2975, 2033, 7078, 19118, 13181, 20200, 999, 2941, 1996, 10479, 3309, 2282, 1045, 1005, 2310, 2042, 1999, 1999, 1996, 1057, 1012, 1055, 999, 1045, 10865, 2000, 1996, 2392, 4624, 1010, 2040, 2059, 2333, 2033, 2000, 1037, 1000, 12882, 1000, 2282, 1011, 1011, 2145, 3243, 2235, 1010, 2021, 6133, 8231, 2061, 1012, 1996, 5723, 2001, 4714, 1010, 2302, 1037, 18736, 1010, 2437, 2009, 3697, 2000, 2404, 2115, 11848, 5134, 1999, 1996, 2282, 1012, 1996, 3295, 3727, 2172, 2000, 2022, 9059, 1010, 2004, 2026, 2282, 2246, 2041, 3495, 3031, 1996, 3449, 1006, 8319, 10798, 1011, 4281, 5005, 1007, 1010, 1998, 1996, 2395, 2001, 3492, 12768, 2006, 1037, 4465, 3944, 1012, 1045, 2097, 2360, 2008, 1996, 2489, 15536, 1011, 10882, 2001, 2307, 1012, 2065, 2017, 2064, 2424, 1037, 4677, 2030, 2060, 3309, 3553, 2000, 4174, 13642, 2638, 1010, 1045, 1005, 1040, 3811, 16755, 2017, 2202, 2008, 1012, 2065, 2017, 2215, 1037, 3835, 2282, 1999, 1037, 2307, 3295, 1010, 4638, 2041, 11175, 2173, 3309, 2612, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# print the encoding for the first training example\n",
    "print(train_encodings.keys())\n",
    "print(train_encodings['input_ids'][3])\n",
    "print(train_encodings['attention_mask'][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a dataset class for the hotel reviews which inherits from torch Dataset\n",
    "\n",
    "class HotelReviewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = HotelReviewsDataset(train_encodings, train_labels)\n",
    "val_dataset = HotelReviewsDataset(val_encodings, val_labels) # dataset for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of labels in the dataset\n",
    "num_labels = len(set(train_labels))\n",
    "\n",
    "# Modify the model to match the number of labels in the dataset\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def validation(validation_dataloader):\n",
    "  with torch.no_grad():\n",
    "    loss_val_list = []\n",
    "    preds_list = []\n",
    "    accuracy_list = []\n",
    "    accuracy_sum = 0\n",
    "    for batch in tqdm(validation_dataloader):\n",
    "      print(batch.keys())\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['labels'].to(device)\n",
    "\n",
    "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "      loss = outputs[0]\n",
    "      logits = F.softmax(outputs[1], dim=1)   # Taking the softmax of output\n",
    "      _,preds = torch.max(logits, dim=1)      # Taking the predictions of our batch\n",
    "      acc = accuracy(logits,labels)           # Calculating the accuracy of current batch\n",
    "      accuracy_sum += acc                     # Taking sum of all the accuracies of all the batches. This sum will be divided by batch length to get mean accuracy for validation dataset\n",
    "\n",
    "      loss_val_list.append(loss)\n",
    "      preds_list.append(preds)\n",
    "      accuracy_list.append(acc)\n",
    "\n",
    "      # for the wrong predictions, print the text\n",
    "      #for i in range(len(labels)):\n",
    "      #  if labels[i] != preds[i]:\n",
    "      #    print(\"Ground truth:\" , convert(labels[i]))\n",
    "      #    print(\"Prediction:\", convert(preds[i]))\n",
    "      #    print(val_texts[i])\n",
    "      #    print(\"----------\")\n",
    "\n",
    "  mean_accuracy = accuracy_sum / len(validation_dataloader)\n",
    "  return mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [06:43<00:00, 16.15s/it]\n",
      "100%|██████████| 25/25 [07:03<00:00, 16.93s/it]\n",
      "100%|██████████| 25/25 [06:39<00:00, 15.99s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    loss_epoch = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optim.zero_grad() # zero out the gradients\n",
    "        input_ids = batch['input_ids'].to(device) # move the batch to the device\n",
    "        attention_mask = batch['attention_mask'].to(device) # move the batch to the device\n",
    "        labels = batch['labels'].to(device) # move the batch to the device\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels) # forward pass\n",
    "        loss = outputs[0] # get the loss\n",
    "        loss_epoch += loss\n",
    "        loss.backward() # backward pass\n",
    "        optim.step() # update the parameters\n",
    "        optim.zero_grad() # zero out the gradients\n",
    "    loss_epoch /= len(train_loader)\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch, loss_epoch))\n",
    "    print(\"Validation accuracy:\", validation(val_loader))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/tokenizer_config.json',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/vocab.txt',\n",
       " './model_save/added_tokens.json',\n",
       " './model_save/tokenizer.json')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`. \n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:06<00:39,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: TRUTHFULPOSITIVE\n",
      "Prediction: DECEPTIVEPOSITIVE\n",
      "This was a weekend trip for my friends and I. We booked a couple of rooms and asked in advanced for the reservations to be connected in some way. Our rooms were right next to one another as requested. As it was almost everyone's first time to Chicago, we had to do a bit of cold-searching for a hotel. And, our choice was next to perfect. I had an amazing time during my first visit to Chicago and it was largely due to this hotel. The customer service was AMAZING. Everyone was extremely nice and helpful. One of our rooms ordered late night room service and raved about how they felt they had ordered a 5 star lunch because it was as fresh as having ordered food at 2 pm, and delicious. My friend suggested they had a full-staff at 3 am because the food came up quickly too. In another of our rooms, housekeeping had accidentally discarded a CD when replacing the CD player with an iPod dock. The concierge had the exact CD purchased and ready at check-out from the neighboring record store. Our rate included complimentary breakfast which was fantastic! Everything was freshly made in their kitchen too. An added bonus was that the hotel is attached to a mall. Therefore, you don't need to leave the building to get a pair of shoes, a coffee, or a some other type of trinket. I was beyond pleased with this hotel and would stay here again without hesitation.\n",
      "----------\n",
      "Ground truth: TRUTHFULNEGATIVE\n",
      "Prediction: DECEPTIVENEGATIVE\n",
      "Wow. Here is a place to NEVER hold an event. First, no one smiles, including the bartenders. Doesn't anyone here know they are in the HOSPITALITY business? The bartender was 54 years old. Why do I know that? Because, she stated to another employee that \"when you get to be 54 years old, you tell it like it is.\" Very specifically, she was referring to the fact that she could tell her employers how they should run things. Wow. The room was set up the wrong way for the event. The food was just plain bad. $9 at a Ponderosa would have been as good. However, this is the Swisshotel. Oh, let's go back to the bartender telling us, unsolicited, about all the problems with the hotel. Man, you just have to wonder how a place like this stays in business. $50 for valet. Ouch. Anyone asking you if you need something, like help? No. Does management make excuses? You bet. Do you ever want to hold an event here? Big no.\n",
      "----------\n",
      "Ground truth: TRUTHFULPOSITIVE\n",
      "Prediction: TRUTHFULNEGATIVE\n",
      "This comes a little late as I'm finally catching up on my reviews from the past several months:) A dear friend and I stayed at the Hyatt Regency in late October 2007 for one night while visiting a friend and her husband from out of town. This hotel is perfect, IMO. Easy check in and check out. Lovely, clean, comfortable rooms with great views of the city. I know this area pretty well and it's very convenient to many downtown Chicago attractions. We had dinner and went clubing with our friends around Division St.. We had no problems getting cabs back and forth to the Hyatt and there's even public transportation right near by but we didn't bother since we only needed cabs from and to the hotel. Parking, as is usual for Chicago, was expensive but we were able to get our car out quickly (however, we left on a Sunday morning, not exactly a high traffic time although it was a Bears homegame day, so a bit busier than usual I would think). No problems at all and the best part is that we got a rate of $100 through Hotwire, a downright steal for this area of Chicago and the quality of the hotel.\n",
      "----------\n",
      "Ground truth: DECEPTIVEPOSITIVE\n",
      "Prediction: TRUTHFULPOSITIVE\n",
      "We stayed for two nights for a meeting. It is an upscale chain hotel and was very clean. The service was very good, as the hotel front desk employees were kind and knowledgeable. The rooms are decent sized and have soft mattresses. The restaurant has good seafood, but was a bit expensive. We would come back again.\n",
      "----------\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:12<00:31,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: TRUTHFULPOSITIVE\n",
      "Prediction: TRUTHFULNEGATIVE\n",
      "This hotel is the best hotel ever in my opinion, and I really enjoy everything thing about it and I also have many different reasons why I like this hotel. From the entrance to every details on the inside all the way up to the rooms, everything seems to pocess some kind of style to it and not forgetting such a modern feel it has. I completely am astonished by this building and if I had to pick one to compare with this one I really would go for this one. It's like a person would have to see it to believe. It has a lot of modern designs. Thank you and this is my review and it's real.\n",
      "----------\n",
      "Ground truth: TRUTHFULPOSITIVE\n",
      "Prediction: DECEPTIVEPOSITIVE\n",
      "After booking a room over two months in advance at the Hotel Sofitel Chicago Watertower and confirming my reservation twice, I arrived to find that the front desk staff had 'no record' of my reservation and that the only rooms available were much more expensive than the room that I had reserved. When I asked to speak to a manager, it took almost half an hour for him to turn up. He did finally give me a room at the rate that I had been promised, but did not offer any compensatory discount or bonuses for my inconvenience. The room itself smelled of dog but was otherwise okay. Not worth the rate though. The website really makes this hotel seem upscale, clean, and well decorated, but the rooms are not in the style that you would expect from the lobby. The staff's response to my request to move to another room because of the obvious dog smell (which could only mean it hadn't been cleaned properly) was met with contempt. They did move me, but I did not get an apology. Will not stay again.\n",
      "----------\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:18<00:25,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: TRUTHFULPOSITIVE\n",
      "Prediction: TRUTHFULNEGATIVE\n",
      "This hotel is the best hotel ever in my opinion, and I really enjoy everything thing about it and I also have many different reasons why I like this hotel. From the entrance to every details on the inside all the way up to the rooms, everything seems to pocess some kind of style to it and not forgetting such a modern feel it has. I completely am astonished by this building and if I had to pick one to compare with this one I really would go for this one. It's like a person would have to see it to believe. It has a lot of modern designs. Thank you and this is my review and it's real.\n",
      "----------\n",
      "Ground truth: TRUTHFULNEGATIVE\n",
      "Prediction: DECEPTIVENEGATIVE\n",
      "While the hotel certainly seems to look beautiful, the hotel is actually far from it. Even booking a room online was rather difficult, and i wasn't able to reach a representative upon calling any of the contact numbers. In general coming here is a bad decision despite how it looks, its a mistake i wont make again and you shouldn't either.\n",
      "----------\n",
      "Ground truth: TRUTHFULPOSITIVE\n",
      "Prediction: TRUTHFULNEGATIVE\n",
      "This comes a little late as I'm finally catching up on my reviews from the past several months:) A dear friend and I stayed at the Hyatt Regency in late October 2007 for one night while visiting a friend and her husband from out of town. This hotel is perfect, IMO. Easy check in and check out. Lovely, clean, comfortable rooms with great views of the city. I know this area pretty well and it's very convenient to many downtown Chicago attractions. We had dinner and went clubing with our friends around Division St.. We had no problems getting cabs back and forth to the Hyatt and there's even public transportation right near by but we didn't bother since we only needed cabs from and to the hotel. Parking, as is usual for Chicago, was expensive but we were able to get our car out quickly (however, we left on a Sunday morning, not exactly a high traffic time although it was a Bears homegame day, so a bit busier than usual I would think). No problems at all and the best part is that we got a rate of $100 through Hotwire, a downright steal for this area of Chicago and the quality of the hotel.\n",
      "----------\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [00:25<00:19,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: DECEPTIVENEGATIVE\n",
      "Prediction: TRUTHFULNEGATIVE\n",
      "Thanks Sheraton Towers for the invite to enjoy your indoor pool while a guest at your hotel recently. I did not know that the skyline of Chicago could be so beautiful or an afternoon at the pool so enjoyable. I'll be back soon and look forward to a seeing Chicago again.\n",
      "----------\n",
      "Ground truth: TRUTHFULNEGATIVE\n",
      "Prediction: TRUTHFULPOSITIVE\n",
      "While the hotel certainly seems to look beautiful, the hotel is actually far from it. Even booking a room online was rather difficult, and i wasn't able to reach a representative upon calling any of the contact numbers. In general coming here is a bad decision despite how it looks, its a mistake i wont make again and you shouldn't either.\n",
      "----------\n",
      "Ground truth: TRUTHFULNEGATIVE\n",
      "Prediction: DECEPTIVENEGATIVE\n",
      "The Palmer House Hilton, while it looks good in pictures, and the outside, is actually a disaster of a hotel. When I went through, the lobby was dirty, my room hadn't been cleaned, and smelled thoroughly of smoke. When I requested more pillows, the lady on the phone scoffed at me and said she'd send them up. It took over an hour for 2 pillows. This hotel is a good example that what you pay for isn't always what you get. I will not be returning.\n",
      "----------\n",
      "Ground truth: DECEPTIVENEGATIVE\n",
      "Prediction: TRUTHFULNEGATIVE\n",
      "Wow. Here is a place to NEVER hold an event. First, no one smiles, including the bartenders. Doesn't anyone here know they are in the HOSPITALITY business? The bartender was 54 years old. Why do I know that? Because, she stated to another employee that \"when you get to be 54 years old, you tell it like it is.\" Very specifically, she was referring to the fact that she could tell her employers how they should run things. Wow. The room was set up the wrong way for the event. The food was just plain bad. $9 at a Ponderosa would have been as good. However, this is the Swisshotel. Oh, let's go back to the bartender telling us, unsolicited, about all the problems with the hotel. Man, you just have to wonder how a place like this stays in business. $50 for valet. Ouch. Anyone asking you if you need something, like help? No. Does management make excuses? You bet. Do you ever want to hold an event here? Big no.\n",
      "----------\n",
      "Ground truth: TRUTHFULPOSITIVE\n",
      "Prediction: DECEPTIVEPOSITIVE\n",
      "My husband and I stayed here at the Hard Rock Hotel in Chicago a few months back. I wasn't particularly impressed with their customer service. When we arrived to check in, the front desk clerk was quite rude and unfriendly. She was short with me when I asked her about the city's attractions and things to do. Our room wasn't all that great either. It was rather small and had a weird smell to it. I won't be staying here again.\n",
      "----------\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:32<00:12,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: TRUTHFULPOSITIVE\n",
      "Prediction: DECEPTIVEPOSITIVE\n",
      "Thanks Sheraton Towers for the invite to enjoy your indoor pool while a guest at your hotel recently. I did not know that the skyline of Chicago could be so beautiful or an afternoon at the pool so enjoyable. I'll be back soon and look forward to a seeing Chicago again.\n",
      "----------\n",
      "Ground truth: TRUTHFULNEGATIVE\n",
      "Prediction: DECEPTIVENEGATIVE\n",
      "My husband and I stayed here at the Hard Rock Hotel in Chicago a few months back. I wasn't particularly impressed with their customer service. When we arrived to check in, the front desk clerk was quite rude and unfriendly. She was short with me when I asked her about the city's attractions and things to do. Our room wasn't all that great either. It was rather small and had a weird smell to it. I won't be staying here again.\n",
      "----------\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [00:39<00:06,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: TRUTHFULNEGATIVE\n",
      "Prediction: DECEPTIVENEGATIVE\n",
      "My husband and I stayed here at the Hard Rock Hotel in Chicago a few months back. I wasn't particularly impressed with their customer service. When we arrived to check in, the front desk clerk was quite rude and unfriendly. She was short with me when I asked her about the city's attractions and things to do. Our room wasn't all that great either. It was rather small and had a weird smell to it. I won't be staying here again.\n",
      "----------\n",
      "Ground truth: TRUTHFULPOSITIVE\n",
      "Prediction: DECEPTIVEPOSITIVE\n",
      "The Ambassador Hotel is located in Downtown Chicago right off Lakeshore Drive in the heart of the hotel industry downtown. I would recommend you look to one of those other hotels if you are wanting a place to stay downtown. The rates at this hotel indicate that it would be a top of the line place to stay, when in fact, we had a pretty unpleasant experience. It started with someone who was obviously a new employee checking us in. I have no problem with this, except the fact that this person was the only one on duty and had difficulties checking us in and had no one to assist him. After checking in, we went up to our room and were a bit disappointed. The pictures do not accurately depict the size of these rooms, they are very tiny and offer very little room to store luggage where it is out of the way. The beds were not very comfortable and when we called room service for extra pillows, they took over an hour to arrive. I will not be staying at this hotel anytime in the future.\n",
      "----------\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:41<00:00,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: TRUTHFULPOSITIVE\n",
      "Prediction: TRUTHFULNEGATIVE\n",
      "This hotel is the best hotel ever in my opinion, and I really enjoy everything thing about it and I also have many different reasons why I like this hotel. From the entrance to every details on the inside all the way up to the rooms, everything seems to pocess some kind of style to it and not forgetting such a modern feel it has. I completely am astonished by this building and if I had to pick one to compare with this one I really would go for this one. It's like a person would have to see it to believe. It has a lot of modern designs. Thank you and this is my review and it's real.\n",
      "----------\n",
      "Ground truth: DECEPTIVEPOSITIVE\n",
      "Prediction: TRUTHFULPOSITIVE\n",
      "Thanks Sheraton Towers for the invite to enjoy your indoor pool while a guest at your hotel recently. I did not know that the skyline of Chicago could be so beautiful or an afternoon at the pool so enjoyable. I'll be back soon and look forward to a seeing Chicago again.\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7679)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
